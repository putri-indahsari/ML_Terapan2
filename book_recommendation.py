# -*- coding: utf-8 -*-
"""book_recommendation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14v8w3FqdAZa-ZI4vhi7WCi9DK3eBqzPE

# **Sistem Rekomendasi Buku Berbasis Hybrid Approach (Content-Based Filtering dan Collaborative Filtering)**

# Sumber Data dan Rujukan
**Dataset:** [Book Recomendation Dataset](https://www.kaggle.com/code/fahadmehfoooz/book-recommendation-system/notebook)

* Dua file utama yang akan digunakan:

  * `Books.csv` — metadata buku (yang berisi ISBN', 'Book-Title', 'Book-Author', 'Year-Of-Publication', 'Publisher',
       'Image-URL-S', 'Image-URL-M', 'Image-URL-L')
  * `user_rating` — hasil merge `Ratings.csv dan Users.csv` yang berisi 'User-ID', 'ISBN', 'Book-Rating', 'Location', 'Age'

# Instal dan Import Library
"""

!pip install numpy==1.26.4

"""menginstal versi tertentu dari library NumPy, yaitu versi 1.26.4,untuk Mencegah error kompatibilitas dengan library surprise."""

!pip install scikit-surprise

"""instal surprice -library yang khusus untuk membangun sistem rekomendasi"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import pickle
sns.set(style='whitegrid')
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from surprise import Reader, Dataset, SVD , accuracy
from surprise.model_selection import train_test_split
from surprise.model_selection import cross_validate
from sklearn.metrics.pairwise import cosine_similarity

from collections import defaultdict
from surprise.model_selection import train_test_split

"""pertama import library yang digunakan:
- pandas : Mengolah dan menganalisis data dalam bentuk dataframe.
- numpy : Mengolah array dan matriks numerik.
- matplotlib.pyplot : Membuat visualisasi data dalam bentuk grafik dan plot.
- seaborn : Membuat visualisasi data yang lebih estetis dan informatif.

"""

from google.colab import drive
drive.mount('/content/drive')

"""menghubungkan ke drive"""

book_df = pd.read_csv('/content/drive/MyDrive/all_data/Books.csv')
ratings_df = pd.read_csv('/content/drive/MyDrive/all_data/Ratings.csv').sample(40000)
user_df = pd.read_csv('/content/drive/MyDrive/all_data/Users.csv')
user_rating_df = ratings_df.merge(user_df, left_on = 'User-ID', right_on = 'User-ID')

"""memanggil data.csv dan melakukan merge(penggabungan data) dataframe ratings_df dan user_df berdasarkan kolom User-ID. Hasilnya disimpan dalam dataframe user_rating_df"""

print('Kolom pada book_df:', book_df.columns)
print('Kolom pada user_rating_df:', user_rating_df.columns)

"""melihat isi kolom book dan user rating

# EDA (Eksplanatory Data Analisis)
"""

print (book_df.head())

"""melihat 5 baris awal dasatet book"""

print(ratings_df.head())

"""5 kolom awal ratings"""

print(user_df.head())

"""melihat 5 kolom awal users"""

print(user_rating_df.head())

"""melihat 5 kolom awal user_rating"""

print("\nbook_df info:")
book_df.info()

print("\nratings_df info:")
ratings_df.info()

print("\nuser_df info:")
user_df.info()

print("\nuser_rating_df info:")
user_rating_df.info()

"""insight

book_df: 271.360 baris data buku
- Tidak ada missing value yang signifikan
- Semua kolom memiliki tipe data object (string)

ratings_df: 40.000 baris data rating (sampel)
- Tidak ada missing value
- Kolom User-ID dan Book-Rating memiliki tipe data integer dan isbn object

user_df: 278.858 baris data pengguna
- Ada missing value pada kolom Age (sekitar 39% data tidak memiliki nilai Age)
- Kolom User-ID memiliki tipe data integer, Age memiliki tipe data float

user_rating_df:
- 40.000 baris data rating yang digabungkan dengan data pengguna
- Ada missing value pada kolom Age (sekitar 27% data tidak memiliki nilai Age)


"""

print("Jumlah missing values di book_df:")
print(book_df.isnull().sum())
print("\nJumlah data duplikat:", book_df.duplicated().sum())

"""insight :
- Kolom Book-Author dan publisher memiliki 2 missing values
- Kolom Image-URL-L memiliki 3 missing values
- Jumlah data duplikat tidak ada data duplikat (0 baris)
"""

print("Jumlah missing values di user_rating_df:")
print(user_rating_df.isnull().sum())
print("\nJumlah data duplikat:", user_rating_df.duplicated().sum())

"""insight
- Kolom Age memiliki 10.724 missing values
- Kolom lainnya tidak memiliki missing values
- Tidak ada data duplikat (0 baris)

"""

plt.figure(figsize=(10, 6))
book_df['Book-Author'].value_counts().head(10).plot(kind='bar')
plt.title('10 Penulis dengan Buku Terbanyak')
plt.xlabel('Penulis')
plt.ylabel('Jumlah Buku')
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()

"""- Top 10 Penulis diisi oleh nama-nama besar dalam dunia fiksi seperti Agatha Christie, Stephen King, dan William Shakespeare."""

plt.figure(figsize=(10, 6))
book_df['Publisher'].value_counts().head(10).plot(kind='bar')
plt.title('10 Penerbit dengan Buku Terbanyak')
plt.xlabel('Penerbit')
plt.ylabel('Jumlah Buku')
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()

"""- Top 10 Penerbit didominasi oleh penerbit-penerbit besar yang terkenal dengan buku fiksi pasar massal seperti Harlequin, Silhouette, dan Pocket."""

rata_rata_rating_buku = user_rating_df['Book-Rating'].mean()
print(f"Rata-rata rating buku: {rata_rata_rating_buku:.2f}")

"""insght :rata-rata rating buku (2.88) menjadi sangat rendah. Perilaku ini harus ditangani secara khusus."""

plt.figure(figsize=(10, 5))
sns.countplot(x='Book-Rating', data=user_rating_df, palette='rocket')
plt.title('Distribusi Rating Buku')
plt.xlabel('Rating')
plt.ylabel('Jumlah')
plt.show()

""" insight : terlihat bahwa rating 0 sangat mendominasi. Dalam konteks dataset ini, rating 0 sering diartikan sebagai rating implisit (pengguna berinteraksi dengan buku tapi tidak memberi skor),Ini perlu jadi pertimbangan saat data preparation."""

plt.figure(figsize=(10, 5))
sns.histplot(user_rating_df['Age'].dropna(), bins=30, kde=True)
plt.title('Distribusi Umur Pengguna')
plt.xlabel('Umur')
plt.ylabel('Frekuensi')
plt.show()

"""insight: terlihat data umur yang tidak realistis (yakni > 100 atau < 5) perlu dibersihkan."""

user_rating_counts = user_rating_df['User-ID'].value_counts()
book_rating_counts = user_rating_df['ISBN'].value_counts()

print(f"Rata-rata rating per pengguna: {user_rating_counts.mean():.2f}")
print(f"Rata-rata rating per buku: {book_rating_counts.mean():.2f}")

"""insight : Rata-rata rating per buku" hanya 1.33(kemungkinan sebagian besar buku dalam dataset ini hanya pernah diberi rating oleh satu atau dua orang)"""

book_df['Year-Of-Publication'] = pd.to_numeric(book_df['Year-Of-Publication'], errors='coerce')
book_df = book_df.dropna(subset=['Year-Of-Publication'])
book_df['Year-Of-Publication'] = book_df['Year-Of-Publication'].astype(int)
print("Tahun-tahun publikasi yang unik (setelah konversi):")
print(sorted(book_df['Year-Of-Publication'].dropna().unique()))

"""- mengubah tipe data Year-Of-Publication yang bertipe object,menjadi angka."""

print("Statistik Deskriptif untuk Kolom Umur:")
print(user_rating_df['Age'].describe())

"""insight :Data usia mengandung informasi berguna, tapi juga memiliki banyak nilai tidak wajar. Oleh karena itu, sebelum digunakan untuk analisis lebih lanjut atau modeling, perlu dilakukan pembersihan (data cleaning) terhadap kolom ini.


"""

correlation_df = pd.merge(user_rating_df, book_df[['ISBN', 'Year-Of-Publication']], on='ISBN', how='left')

cleaned_corr_df = correlation_df.dropna(subset=['Year-Of-Publication', 'Age']) # Hapus baris dgn NaN di kolom ini
cleaned_corr_df = cleaned_corr_df[
    (cleaned_corr_df['Year-Of-Publication'] >= 1900) &
    (cleaned_corr_df['Year-Of-Publication'] <= 2025)
]

cleaned_corr_df = cleaned_corr_df[
    (cleaned_corr_df['Age'] >= 5) &
    (cleaned_corr_df['Age'] <= 100)
]

print(f"Jumlah data setelah dibersihkan: {len(cleaned_corr_df)}")

numerical_cols = cleaned_corr_df[['Age', 'Book-Rating', 'Year-Of-Publication']]

correlation_matrix = numerical_cols.corr()

plt.figure(figsize=(8, 6))
sns.heatmap(
    correlation_matrix,
    annot=True,
    cmap='coolwarm',
    fmt=".2f"
)
plt.title('Heatmap Korelasi Antar Variabel Numerik')
plt.show()

print(correlation_matrix)

"""- Age vs Book-Rating: -0.02 : Umur pengguna tidak menentukan apakah mereka akan memberi rating tinggi atau rendah. Pengguna muda, dewasa, maupun yang lebih tua memiliki kecenderungan rating yang serupa.
- Age vs Year-Of-Publication: -0.02: Tidak ada tren yang menunjukkan bahwa pengguna dari kelompok umur tertentu lebih menyukai buku-buku dari era tertentu (misalnya, pengguna muda tidak secara signifikan lebih banyak membaca buku baru).
- Book-Rating vs Year-Of-Publication: 0.05: Buku-buku baru tidak secara otomatis mendapatkan rating yang lebih tinggi dibandingkan buku-buku yang lebih tua, dan sebaliknya.

---

# **Data Preparation**

## Membersihkan Data Outlier dan Tidak Valid
"""

print(f"Jumlah baris book_df sebelum dibersihkan: {len(book_df)}")

"""* melihat jumlah baris sebelum dibersihkan"""

book_df['Year-Of-Publication'] = pd.to_numeric(book_df['Year-Of-Publication'], errors='coerce')

"""* Mengubah 'Year-Of-Publication' ke numerik, error menjadi NaN"""

book_df.dropna(subset=['Year-Of-Publication'], inplace=True) # Hapus baris dengan NaN di tahun
book_df = book_df.loc[
    (book_df['Year-Of-Publication'] >= 1900) &
    (book_df['Year-Of-Publication'] <= 2025)
]

"""*  memfilter tahun yang valid (1900 - 2025) dan hapus NaN"""

book_df['Year-Of-Publication'] = book_df['Year-Of-Publication'].astype(int)
print(f"Jumlah baris book_df setelah dibersihkan: {len(book_df)}")

"""*  Jumlah baris book_df setelah dibersihkan: 266723 dan konversi ke integer setelah dibersihkan"""

user_rating_df.dropna(subset=['Age'], inplace=True) # Hapus baris dengan NaN di umur
user_rating_df = user_rating_df.loc[
    (user_rating_df['Age'] >= 5) &
    (user_rating_df['Age'] <= 100) ]

"""* Pada kolom age data yang kosong dihapus (baris dengan NaN pada kolom Age dihapus). Setelah itu, dilakukan pembersihan outlier, yaitu usia di luar rentang 5 hingga 100 tahun.

## Preprocessing untuk Model Content-Based Filtering
"""

book_df.isnull().sum()
book_df.dropna(subset=['Book-Author', 'Publisher'], inplace=True)

"""* Cek missing values dan hapus baris jika 'Book-Author' atau 'Publisher' kosong"""

book_df['features'] = book_df['Book-Author'] + ' ' + book_df['Publisher']

print("Contoh fitur gabungan:")
print(book_df[['Book-Author', 'Publisher', 'features']].head())

book_content_df = book_df.copy()

"""* Gabungkan fitur penulis dan penerbit menjadi satu kolom 'features' dan menyimpan data yang siap pakai untuk model ini `book_content_df = book_df.copy()`

## Preprocessing untuk Model Collaborative Filtering
"""

explicit_ratings_df = user_rating_df[user_rating_df['Book-Rating'] != 0]
print(f"Jumlah rating eksplisit: {len(explicit_ratings_df)}")

"""* Memisahkan Rating Eksplisit dengan membuat dataframe baru, menggunakan rating yang jelas (1-10) untuk mengetahui preferensi pengguna."""

user_counts = explicit_ratings_df['User-ID'].value_counts()
print("Statistik Jumlah Rating per Pengguna:")
print(user_counts.describe())
print("\nContoh 5 pengguna paling aktif:")
print(user_counts.head(5))

print("-" * 30)

book_counts = explicit_ratings_df['ISBN'].value_counts()
print("\nStatistik Jumlah Rating per Buku:")
print(book_counts.describe())
print("\nContoh 5 buku paling populer:")
print(book_counts.head(5))

"""* Cek Distribusi Rating untuk menentukan angka threshold dengan pengguna yang paling aktif"""

MIN_RATINGS_PER_USER = 5
MIN_RATINGS_PER_BOOK = 2

user_counts = explicit_ratings_df['User-ID'].value_counts()
active_users = user_counts[user_counts >= MIN_RATINGS_PER_USER].index
filtered_df = explicit_ratings_df[explicit_ratings_df['User-ID'].isin(active_users)]

book_counts = filtered_df['ISBN'].value_counts()
popular_books = book_counts[book_counts >= MIN_RATINGS_PER_BOOK].index
final_df = filtered_df[filtered_df['ISBN'].isin(popular_books)]

print(f"Bentuk data sebelum filtering: {explicit_ratings_df.shape}")
print(f"Bentuk data setelah filtering dengan threshold baru: {final_df.shape}")

"""* memfilter pengguna dan buku berdasarkan jumlah rating.Hasilnya 175 interaksi yang padat dan berkualitas tinggi daripada pada 10.557 interaksi yang sebagian besar "terputus-putus" dan acak."""

collaborative_df = final_df.copy()

print("Variabel 'collaborative_df' berhasil dibuat")
print(f"Bentuk data: {collaborative_df.shape}")

"""* Simpan hasil ke variabel 'collaborative_df

-----

# **Modeling**

# Content-Based Filtering  
##(Merekomendasikan buku berdasarkan kemiripan atribut/konten buku)
"""

popular_isbns = collaborative_df['ISBN'].unique()
content_filtered_df = book_content_df[book_content_df['ISBN'].isin(popular_isbns)].copy()

"""* menyaring data buku agar content-based filtering hanya bekerja pada buku-buku populer, yaitu buku-buku yang sudah lolos threshold di collaborative filtering"""

print("Informasi book_content_df:")
print(book_content_df.info())

print("\n5 baris pertama book_content_df:")
print(book_content_df.head())

print("\nJumlah missing values di book_content_df:")
print(book_content_df.isnull().sum())

"""melihat informasi book_content_df untuk memastikan data yang digunakan pemodelan yang selanjutnya"""

tfidf = TfidfVectorizer()
tfidf_matrix = tfidf.fit_transform(book_content_df['features'])

print(tfidf_matrix.shape)

"""* Setiap buku diwakili oleh vektor dengan 54.064 dimensi, di mana tiap dimensi merepresentasikan kata unik (unik dari kombinasi Book-Author + Publisher).
* 266.719 merupakan Jumlah kolom /jumlah fitur unik dari teks (vocabulary)
insight : Matrix besar sekali (266rb × 54rb) → ini yang bisa menyebabkan kehabisan RAM lanjut ke cosine similarity atau evaluasi.Itulah sebabnya diawal perlu menyaring buku menjadi content_filtered_df di tahap sebelumnya
"""

print(collaborative_df.info())
print(collaborative_df.head())
print(collaborative_df.isnull().sum())

"""* melihat dan memastikan data collaborative_df ada untuk tahap selanjutnya"""

reader = Reader(rating_scale=(1, 10))
data = Dataset.load_from_df(collaborative_df[['User-ID', 'ISBN', 'Book-Rating']], reader)

trainset = data.build_full_trainset()
svd = SVD()
svd.fit(trainset)

print("\nModel SVD berhasil dilatih.")

"""* menyiapkan data untuk library 'surprise'
Inisialisasi Reader untuk membaca data, dengan skala rating 1-10
"""

def get_user_recommendations(user_id, model=svd, n=5):
    """
    Fungsi untuk memberikan rekomendasi buku kepada pengguna.
    """
    all_isbns = collaborative_df['ISBN'].unique()
    rated_isbns = collaborative_df[collaborative_df['User-ID'] == user_id]['ISBN'].unique()
    unrated_isbns = [isbn for isbn in all_isbns if isbn not in rated_isbns]
    predictions = [model.predict(user_id, isbn) for isbn in unrated_isbns]
    predictions.sort(key=lambda x: x.est, reverse=True)
    top_n_preds = predictions[:n]
    recommended_isbns = [pred.iid for pred in top_n_preds]
    recommended_books = book_content_df[book_content_df['ISBN'].isin(recommended_isbns)]

    return recommended_books[['Book-Title', 'Book-Author']]

"""4. Latih Model SVD
Inisialisasi dan latih model SVD, Bangun trainset dari keseluruhan data
'''
"""

test_user_id = collaborative_df['User-ID'].iloc[0]
user_recs = get_user_recommendations(test_user_id)

print(f"\n>>> Rekomendasi buku untuk User-ID {test_user_id}:")
print(user_recs)

"""buku yang direkomendasikan:
- Fast Food Nation...: Non-Fiksi, Jurnalisme Investigatif.
- The Stand,..: Horor/Fantasi Apokaliptik.
- All Around the Town: Suspense/Misteri.
- The Fairy Godmother: Fantasi.
- Would You Like to Play Hide & Seek...: Buku Anak-anak.
semua genre berbeda yang menunjukkan hasil model tidak melihat konten sama sekali, melainkan murni dari pola rating.Mungkin ada contoh pengguna yang membaca Stephen King untuk dirinya sendiri dan Jon Stone untuk anaknya, dan model menangkap ini sebagai sebuah pola
"""

available_users = collaborative_df['User-ID'].unique()
print(f"Beberapa User-ID yang tersedia: {available_users[:10]}")

"""* Lihat beberapa User-ID unik yang tersedia di dalam data"""

if len(available_users) > 5:
    another_test_user_id = available_users[3]

    another_user_recs = get_user_recommendations(another_test_user_id)

    print(f"\n>>> Rekomendasi buku untuk User-ID {another_test_user_id}:")
    print(another_user_recs)
else:
    print("\nTidak cukup pengguna dalam data untuk memilih contoh lain.")

"""* Uji  Rekomendasi untuk Pengguna Lain :mengganti available_users[2] dengan indeks lain atau User-ID spesifik dari daftar yang ditampilkan untuk melihat hasil yang berbeda.


"""

tfidf = TfidfVectorizer()
tfidf_matrix_filtered = tfidf.fit_transform(content_filtered_df['features'])

"""* Buat TF-IDF dari content_filtered_df untuk melihat similarity"""

def content_based_recommend(book_title, top_n=5):
    idx = content_filtered_df[content_filtered_df['Book-Title'] == book_title].index[0]
    cos_sim = cosine_similarity(tfidf_matrix_filtered[idx], tfidf_matrix_filtered).flatten()
    similar_indices = cos_sim.argsort()[-top_n-1:-1][::-1]

    recommended_books = content_filtered_df.iloc[similar_indices][['Book-Title', 'Book-Author']]
    recommended_books['Similarity Score'] = cos_sim[similar_indices]

    return recommended_books

"""* Ambil index buku dari content_filtered_df lalu menghitung cosine similarity dan ambil buku dari top 1 yang mirip"""

content_filtered_df['Book-Title'].sample(10)

"""* melihat sample untuk uji coba similarity"""

cbf_recs = content_based_recommend('Love You Forever', top_n=5)
print(cbf_recs)

"""- Model berhasil merekomendasikan buku-buku lain yang umumnya termasuk dalam genre fiksi dewasa dan relasi keluarga atau hubungan emosional, yang selaras dengan tema dari Love You Forever, sebuah buku yang berkisah tentang hubungan kasih sayang orang tua dan anak.

- Meskipun tidak semua buku yang direkomendasikan memiliki tema yang sama persis, pemilihan penulis populer seperti Nora Roberts dan John Grisham menunjukkan bahwa model menangkap popularitas dan relevansi pasar dari buku-buku tersebut.

- Skor kemiripan cukup rendah (~0.05), yang mengindikasikan bahwa fitur yang digunakan (author + publisher) masih kurang kuat untuk menghasilkan kemiripan yang lebih tajam — hal ini memberikan ruang pengembangan lebih lanjut dengan menambah fitur seperti genre atau sinopsis.

# Collaborative Filtering
## ( Merekomendasikan buku berdasarkan kesamaan selera antar pengguna.)
"""

print("Mengevaluasi model SVD dengan 5-Fold Cross-Validation...")
cross_validate(svd, data, measures=['RMSE', 'MAE'], cv=5, verbose=True)

"""- Lakukan 5-fold cross-validation untuk mengevaluasi model SVD
- ita akan mengukur RMSE dan MAE
- cv=5' berarti data akan dibagi menjadi 5 bagian, diuji 5 kali
- verbose=True' akan menampilkan prosesnya

insight :
Akurasi Model memiliki performa yang solid dengan nilai rata-rata RMSE sebesar 1.7470.Secara rata-rata, prediksi rating yang diberikan oleh model meleset sekitar 1.74 poin dari rating sebenarnya (pada skala 1-10).

Kesimpulan: Mengingat data yang digunakan sangat sparse (jarang), hasil ini sangat baik. Ini membuktikan bahwa model berhasil mempelajari pola preferensi pengguna secara efektif dan kinerjanya jauh lebih baik daripada tebakan acak.
"""

def get_user_recommendations(user_id, model=svd, n=5):
    all_isbns = collaborative_df['ISBN'].unique()
    rated_isbns = collaborative_df[collaborative_df['User-ID'] == user_id]['ISBN'].unique()

    unrated_isbns = [isbn for isbn in all_isbns if isbn not in rated_isbns]

    predictions = [model.predict(user_id, isbn) for isbn in unrated_isbns]

    predictions.sort(key=lambda x: x.est, reverse=True)

    top_n_preds = predictions[:n]

    recommended_isbns = [pred.iid for pred in top_n_preds]
    predicted_ratings = [pred.est for pred in top_n_preds]

    recommended_books = book_content_df[book_content_df['ISBN'].isin(recommended_isbns)][['Book-Title', 'Book-Author']].reset_index(drop=True)
    recommended_books['Predicted Rating'] = predicted_ratings[:len(recommended_books)]

    return recommended_books

"""* menggunakan fungsi get_user_recommendations memberikan rekomendasi buku berdasarkan preferensi pengguna. Hasilnya nanti mencantumkan Judul, Penulis, dan Predicted Rating"""

available_users = collaborative_df['User-ID'].unique()
print(f"User-ID yang tersedia: {available_users[:10]}")

test_user_id = available_users[2]  # conoth user
user_recs = get_user_recommendations(test_user_id, n=5)

print(f"Rekomendasi untuk User-ID {test_user_id}:")
print(user_recs)

"""Insight:

- Model berhasil merekomendasikan buku-buku yang sangat populer dari berbagai genre klasik dan fiksi populer.

- Ini menunjukkan bahwa Collaborative Filtering SVD mampu menangkap preferensi pengguna berdasarkan pola rating pengguna lain yang memiliki kesamaan minat.

- Rekomendasi mencerminkan kemampuan model untuk menghadirkan buku lintas genre, bukan hanya berdasarkan konten, melainkan berdasarkan pola rating pengguna-pengguna serupa (serendipity).

- Predicted Rating tinggi (di atas 8.6) menunjukkan bahwa model cukup yakin pengguna ini akan menyukai buku-buku yang direkomendasikan.
"""

with open('svd_model.pkl', 'wb') as file:
    pickle.dump(svd, file)

print("Model SVD berhasil disimpan ke dalam file 'svd_model.pkl'")

"""menyimpan model svd dan tidak menyimpan model pertama karena ram tidak mencukupi"""

collaborative_df.to_csv('collaborative_data.csv', index=False)
book_content_df.to_csv('book_content_data.csv', index=False)

print("DataFrame yang dibutuhkan juga telah disimpan.")

"""menyimpan dataframe hasil pemodelan sebelumnya yang siap digunakan untuk pemodelan.

## Perbedaan kedua model

1. Content-Based Filtering: Kekuatan Relevansi yang Aman

Model Content-Based, yang bekerja berdasarkan kemiripan atribut buku (penulis dan penerbit), terbukti efektif dalam menemukan rekomendasi yang relevan secara langsung dan dapat diprediksi. Dengan metrik evaluasi Precision@5 yang menunjukkan kemampuan model dalam merekomendasikan buku dari penulis yang sama, model ini unggul dalam memberikan rekomendasi yang "aman".

- Kekuatan Utama: Transparansi dan keandalan. Jika seorang pengguna menyukai buku dari Stephen King, model ini dengan sangat baik akan merekomendasikan buku Stephen King lainnya. Ini memenuhi kebutuhan pengguna yang ingin "lebih banyak yang seperti ini".
- Kelemahan: Model ini menciptakan "filter bubble" atau gelembung filter, di mana pengguna akan terus berada di zona nyaman seleranya tanpa pernah menemukan genre atau penulis baru yang mungkin mereka sukai.
2. Collaborative Filtering: Kekuatan Penemuan Tak Terduga (Serendipity)

Model Collaborative Filtering, yang menggunakan algoritma SVD pada pola rating pengguna, menunjukkan hasil yang sangat berbeda. Dengan RMSE rata-rata sebesar 1.68, model ini memiliki akurasi prediksi yang terukur. Namun, kekuatan sejatinya terletak pada kualitas rekomendasinya.

- Kekuatan Utama: Kemampuan serendipity. Seperti yang kita lihat pada contoh hasil, model ini mampu merekomendasikan buku dari genre yang sangat beragam (misalnya, non-fiksi, horor, hingga buku panduan game) dalam satu daftar. Ini terjadi karena model belajar dari "kearifan kolektif" (collective wisdom) dari semua pengguna, menemukan koneksi antar item yang tidak terduga dan tidak akan pernah ditemukan oleh analisis konten.
- Kelemahan: Menderita cold-start problem (sulit memberi rekomendasi untuk pengguna/buku baru) dan sensitif terhadap "noise" atau pola aneh dalam data, terutama pada dataset yang kecil.

Kesimpulan Komparatif:

- Content-Based Filtering lebih baik untuk memperdalam minat pengguna yang sudah ada.
- Collaborative Filtering lebih baik untuk memperluas wawasan dan meningkatkan engagement pengguna dengan membantu mereka menemukan hal-hal baru.

Dalam aplikasi dunia nyata, sistem rekomendasi paling canggih seperti pada Netflix atau Amazon tidak memilih salah satu, melainkan menggunakan pendekatan hybrid. Mereka menggabungkan kekuatan keduanya untuk memberikan daftar rekomendasi yang seimbang antara yang "aman dan relevan" dengan yang "baru dan mengejutkan",ini telah berhasil menunjukkan bahwa kedua model dapat dibangun dan dievaluasi secara efektif untuk menyelesaikan masalah awal. Keduanya mampu menyaring jutaan pilihan menjadi daftar pendek yang dipersonalisasi, dan masing-masing memberikan nilai unik bagi pengalaman pengguna.

---

# Evaluasi

# Inference
"""

tfidf = TfidfVectorizer()
tfidf_matrix = tfidf.fit_transform(content_filtered_df['features'])

"""* ini mengambil kolom teks features (yang berisi gabungan penulis dan penerbit) dan mengubahnya menjadi sebuah matriks angka raksasa yang disebut tfidf_matrix.`tfidf_matrix` ini sekarang merepresentasi matematis dari seluruh katalog buku"""

def calculate_content_precision_memory_efficient(df, tfidf_matrix, k=5, sample_size=20):
    """
    Menghitung rata-rata Precision@k secara hemat memori.
    """
    if len(df) < sample_size:
        sample_size = len(df)

    sample_df = df.sample(n=sample_size, random_state=42)
    indices_map = pd.Series(df.index, index=df['Book-Title'])

    total_precision = 0

    for _, row in sample_df.iterrows():
        input_title = row['Book-Title']
        input_author = row['Book-Author']

        # Dapatkan index dari buku input
        try:
            idx = indices_map[input_title]
        except KeyError:
            continue

"""menyiapkan fungsi evaluasi diantara tiga hal: 1. Mengambil sampel acak sebanyak 20 buku (sample_df) untuk diuji, 2. Membuat indices_map untuk mencari posisi sebuah buku dengan cepat berdasarkan judulnya, dan 3. Menyiapkan blok try...except untuk menangani jika ada judul buku yang tidak ditemukan.

"""

def calculate_content_precision_memory_efficient(df, tfidf_matrix, k=5, sample_size=20):
    from sklearn.metrics.pairwise import cosine_similarity

    if len(df) < sample_size:
        sample_size = len(df)

    sample_df = df.sample(n=sample_size, random_state=42)
    indices_map = pd.Series(df.index, index=df['Book-Title'])

    total_precision = 0
    count_valid = 0

    for _, row in sample_df.iterrows():
        input_title = row['Book-Title']
        try:
            idx = indices_map[input_title]
        except KeyError:
            continue

        cosine_sim = cosine_similarity(tfidf_matrix[idx], tfidf_matrix).flatten()
        sim_scores = list(enumerate(cosine_sim))
        sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)

        # Ambil k+1 karena item pertama adalah dirinya sendiri
        sim_scores = sim_scores[1:k+1]
        recommended_indices = [i[0] for i in sim_scores]

        recommended_authors = df.iloc[recommended_indices]['Book-Author'].values
        true_author = row['Book-Author']

        # Precision: seberapa banyak dari k hasil yang penulisnya sama dengan input
        relevant = sum([1 for a in recommended_authors if a == true_author])
        precision = relevant / k
        total_precision += precision
        count_valid += 1

    avg_precision = total_precision / count_valid if count_valid > 0 else 0
    print(f'Precision@{k} untuk Content-Based Filtering: {avg_precision:.4f}')

"""- Fungsi ini menghitung Precision@k untuk Content-Based Filtering.Kemudian Sistem memilih 20 sampel buku secara acak dari dataset yang sudah difilter.

- Untuk tiap buku, dihitung kemiripan (cosine similarity) dengan seluruh buku lainnya berdasarkan vektor TF-IDF lalu diambil k buku paling mirip (tanpa termasuk dirinya sendiri).

- Relevansi dievaluasi berdasarkan kesamaan penulis dengan buku awal.

- Precision dihitung sebagai rasio buku relevan dalam top-k.Rata-rata precision dari semua sampel ditampilkan sebagai Precision@k akhir.
"""

def calculate_content_precision_memory_efficient(df, tfidf_matrix, k=5, sample_size=20):
    from sklearn.metrics.pairwise import cosine_similarity

    # Reset index agar indeks tfidf_matrix sejajar dengan df
    df = df.reset_index(drop=True)

    if len(df) < sample_size:
        sample_size = len(df)

    sample_df = df.sample(n=sample_size, random_state=42)
    indices_map = pd.Series(df.index, index=df['Book-Title'])

    total_precision = 0
    count_valid = 0

    for _, row in sample_df.iterrows():
        input_title = row['Book-Title']
        input_author = row['Book-Author']

        try:
            idx = indices_map[input_title]
        except KeyError:
            continue

        cosine_sim = cosine_similarity(tfidf_matrix[idx], tfidf_matrix).flatten()
        sim_scores = list(enumerate(cosine_sim))
        sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)
        sim_scores = sim_scores[1:k+1]  # Skip self

        recommended_indices = [i[0] for i in sim_scores]
        recommended_authors = df.iloc[recommended_indices]['Book-Author'].values
        true_author = row['Book-Author']

        relevant = sum([1 for a in recommended_authors if a == true_author])
        precision = relevant / k
        total_precision += precision
        count_valid += 1

    avg_precision = total_precision / count_valid if count_valid > 0 else 0
    print(f'Precision@{k} untuk Content-Based Filtering: {avg_precision:.4f}')

"""Pada tahap evaluasi sistem Content-Based Filtering, dilakukan perhitungan metrik Precision@k untuk menilai relevansi rekomendasi yang dihasilkan. Fungsi yang digunakan mengambil sejumlah buku sebagai sampel uji (sebanyak 20 buku), lalu menghitung kemiripan konten antar buku menggunakan teknik cosine similarity berdasarkan representasi vektor dari fitur gabungan (Book-Author dan Publisher) yang telah diproses dengan TF-IDF. Dari hasil perhitungan tersebut, sistem memilih k buku teratas yang paling mirip sebagai rekomendasi. Relevansi ditentukan berdasarkan kesamaan penulis antara buku input dengan hasil rekomendasi — jika penulis sama, rekomendasi dianggap relevan. Precision dihitung sebagai rasio item relevan terhadap jumlah total rekomendasi (k), lalu dirata-ratakan dari seluruh sampel untuk memperoleh nilai Precision@k akhir. Hasil evaluasi ini menunjukkan seberapa baik sistem dalam memberikan rekomendasi konten yang dianggap serupa, meskipun dalam proyek ini, pendekatan berbasis konten masih sederhana dan hanya mempertimbangkan penulis dan penerbit sebagai fitur utama.


"""

content_filtered_df = content_filtered_df.reset_index(drop=True)

"""* melakukan reset_index() pada content_filtered_df agar indeksnya sesuai dengan urutan pada tfidf_matrix. Ini penting agar proses pengambilan data berdasarkan indeks tidak menyebabkan error saat perhitungan cosine similarity."""

calculate_content_precision_memory_efficient(content_filtered_df, tfidf_matrix, k=5, sample_size=20)

"""* Menggunakan fungsi calculate_content_precision_memory_efficient, sistem menghitung nilai Precision@5 berdasarkan kemiripan antar buku dari vektor TF-IDF. Evaluasi ini menunjukkan bahwa hanya 4% dari top-5 rekomendasi yang dianggap relevan, menandakan model masih perlu ditingkatkan."""

cross_validate(svd, data, measures=['RMSE', 'MAE'], cv=5, verbose=True)

"""Model SVD dievaluasi menggunakan cross_validate() dengan 5-fold cross-validation dan metrik RMSE serta MAE. Hasil evaluasi menunjukkan performa prediksi model yang cukup baik dan stabil dalam memperkirakan rating pengguna terhadap buku.

**Metrik Evaluasi:**

1. Root Mean Squared Error (RMSE)
RMSE mengukur akar dari rata-rata kuadrat selisih antara rating prediksi dan rating aktual. Nilai RMSE yang lebih rendah menunjukkan bahwa model memberikan prediksi yang lebih akurat. Metrik ini sensitif terhadap kesalahan besar dan digunakan untuk mengukur performa keseluruhan dari model.

2. Mean Absolute Error (MAE)
MAE menghitung rata-rata dari nilai absolut selisih antara rating yang diprediksi dan nilai aktual. MAE memberikan gambaran seberapa jauh prediksi berada dari nilai sebenarnya secara umum, tanpa memperbesar efek outlier seperti pada RMSE.

**Hasil Evaluasi (menggunakan 5-Fold Cross-Validation):**
- RMSE: 1.8524
- MAE: 1.4245
"""

trainset, testset = train_test_split(data, test_size=0.2)
svd.fit(trainset)
predictions = svd.test(testset)

def precision_at_k(predictions, k=5, threshold=7):
    """Menghitung Precision@k"""
    top_n = defaultdict(list)

    for uid, iid, true_r, est, _ in predictions:
        top_n[uid].append((iid, est))

    # Urutkan dan ambil top-k
    for uid, user_ratings in top_n.items():
        user_ratings.sort(key=lambda x: x[1], reverse=True)
        top_n[uid] = user_ratings[:k]

    precisions = []

    for uid, user_ratings in top_n.items():
        n_rel = sum((est >= threshold) for (_, est) in user_ratings)
        precisions.append(n_rel / k)

    return sum(precisions) / len(precisions)

p_at_5 = precision_at_k(predictions, k=5, threshold=7)
print(f'Precision@5 untuk Collaborative Filtering (SVD): {p_at_5:.4f}')

"""Analisi menggunakan metrik Precision@5 untuk mengukur relevansi rekomendasi dalam konteks top-N recommendation. Model dilatih menggunakan 80% data sebagai trainset dan diuji pada 20% sisanya (testset). Untuk setiap pengguna, sistem memilih lima buku dengan prediksi rating tertinggi, dan item dinilai relevan jika prediksinya ≥ 7. Precision dihitung sebagai proporsi item relevan dari total top-5 rekomendasi dan dirata-ratakan untuk seluruh pengguna.

Hasil evaluasi menunjukkan bahwa model SVD berhasil mencapai Precision@5 sebesar 0.2000, yang berarti sekitar 20.00% dari rekomendasi yang diberikan dianggap relevan. Meskipun tidak sangat tinggi, nilai ini menunjukkan bahwa model mampu memberikan rekomendasi yang cukup baik dan lebih relevan dibandingkan pendekatan content-based sederhana, terutama dalam konteks dataset yang cukup sparse.
"""

with open('best_model.pkl', 'wb') as f:
    pickle.dump(svd, f)

print("Model SVD berhasil disimpan sebagai 'best_model.pkl'")

"""Model disimpan dalam format .pkl"""